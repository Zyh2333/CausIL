{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "sys.path.append('../python/structure_learning/constraint_based/')\n",
    "from mdepcs import MDEPCS\n",
    "from mmpc import MMPC\n",
    "from pc import pcalg\n",
    "sys.path.append('../python/structure_learning/score_based/')\n",
    "from hc import hill_climb\n",
    "from fges import fges\n",
    "sys.path.append('../python/')\n",
    "from scores import *\n",
    "from ci_tests import *\n",
    "from discretize import *\n",
    "from bnutils import *\n",
    "sys.path.append('../libraries/')\n",
    "from caim_test import get_caim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################   INPUT    ######################################################\n",
    "data = pd.read_pickle('/home/code-base/gitrepo/Mixed/python/mixed_data/10nodes13edges/data_0.gpickle')\n",
    "max_categories = 20\n",
    "constraint_based = MDEPCS  # MDEPCS(default- works with fcit only), MMPC\n",
    "algo = 'hybrid' #'hybrid'(default), 'constraint', 'score'\n",
    "# disc = 'quantile' #For univariate only(quantile(default), uniform, kmeans)\n",
    "# bins = 5 #For univariate only(quantile(default), uniform, kmeans)\n",
    "alpha = 0.1 # (0.05 for PC, MMPC) (0.1 for MDEPCS only)\n",
    "ci_test = fast_conditional_ind_test # fast_conditional_ind_test(default), chi_square_test\n",
    "verbose = False\n",
    "max_processes = 40\n",
    "discretizer = Data_Driven_Discretizer #Data_Driven_Discretizer(For CAIM, LAIM, AMEVA), unsupervised_discretization\n",
    "disc_score = get_laim #get_mlameva(default), get_laim, get_caim, get_ameva\n",
    "search_and_score = fges #For hybrid and score only,(fges(default)), hill_climb)\n",
    "score = bdeu_score #For hybrid only(bdeu_score, bic_score_discrete)\n",
    "result_score = bic_score_discrete\n",
    "custom = True  ## flag to specify type through cont_list or autoselect column types if false\n",
    "cont_columns = [] ## specify list of continuous columns here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(data):\n",
    "    \"\"\"Runner for different types of algo\n",
    "    \n",
    "    Returns:\n",
    "        dag : the result graph from the algo\n",
    "        data : discretized data\n",
    "        score of the graph\n",
    "    \"\"\"\n",
    "    ######################################################    DATA PREPROCESSING    ######################################################\n",
    "    \n",
    "    nodes = []\n",
    "    args = nx.DiGraph()\n",
    "    mappers = column_mapping(data)\n",
    "    data.rename(columns = mappers[0],inplace=True)\n",
    "    args.add_nodes_from(list(data.columns))\n",
    "    if not custom:\n",
    "        for col in data.columns:\n",
    "            categories = len(pd.Series.unique(data[col]))\n",
    "            if(categories > max_categories):\n",
    "                args.nodes[col]['type'] = 'cont'\n",
    "                args.nodes[col]['num_categories'] = 'NA'\n",
    "            else:\n",
    "                args.nodes[col]['type'] = 'disc'\n",
    "                args.nodes[col]['num_categories'] = categories\n",
    "                data = data.replace({col: pd.unique(data[col])}, {col: list(range(pd.unique(data[col]).shape[0]))})        \n",
    "    else:\n",
    "        for col in data.columns:\n",
    "            categories = len(pd.Series.unique(data[col]))\n",
    "            if mappers[1][col] in cont_columns:\n",
    "                args.nodes[col]['type'] = 'cont'\n",
    "                args.nodes[col]['num_categories'] = 'NA'\n",
    "            else:\n",
    "                args.nodes[col]['type'] = 'disc'\n",
    "                args.nodes[col]['num_categories'] = categories\n",
    "                data = data.replace({col: pd.unique(data[col])}, {col: list(range(pd.unique(data[col]).shape[0]))})        \n",
    "    \n",
    "    CBE = ConstraintBasedEstimator(data)\n",
    "    \n",
    "    if algo == 'constraint':\n",
    "        dag = CBE.pdag_to_dag(pcalg(data, args.nodes(data=True), alpha, ci_test, disc, bins))\n",
    "        data.rename(columns = mappers[1], inplace = True)\n",
    "        nx.relabel_nodes(dag, mappers[1], copy=False)\n",
    "        return (dag, data, result_score(dag, data))\n",
    "    elif algo == 'score':\n",
    "        dag = CBE.pdag_to_dag(fges(data, args.nodes(data=True), disc = disc, n_bins = bins, score = score))\n",
    "        data.rename(columns = mappers[1], inplace = True)\n",
    "        nx.relabel_nodes(dag, mappers[1], copy=False)\n",
    "        return (dag, data, result_score(dag, data))\n",
    "    \n",
    "    ######################################################   SKELETON LEARNING   ######################################################\n",
    "    disc_data = None\n",
    "    if discretizer == unsupervised_discretization:\n",
    "        disc_data = discretizer(data.copy(), [node[0] for node in args.nodes(data=True) if node[1]['type'] == 'cont'], bins, disc)\n",
    "    \n",
    "    if constraint_based == MDEPCS: \n",
    "        skel = constraint_based(data, args.nodes(data=True), alpha, ci_test, verbose, max_processes).mdepcs()\n",
    "    elif constraint_based == MMPC:\n",
    "        skel = constraint_based(data, args.nodes(data=True), alpha, ci_test, verbose, max_processes).mmpc()\n",
    "    ######################################################   DISCRETIZATION   ######################################################\n",
    "    \n",
    "    if discretizer == Data_Driven_Discretizer:\n",
    "        disc_data = discretizer(data.copy(), skel, args.nodes(data=True), alpha, max_processes, method=disc_score).discretize()\n",
    "        \n",
    "    ######################################################   SEARCH AND SCORE   ######################################################\n",
    "    dag = CBE.pdag_to_dag(search_and_score(disc_data, args.nodes(data=True), score = score))\n",
    "    disc_data.rename(columns = mappers[1],inplace=True)\n",
    "    nx.relabel_nodes(dag, mappers[1], copy=False)\n",
    "    return (dag, disc_data, result_score(dag, disc_data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical []\n",
      "# 0  GLOBAL CAIM  0.05354955699322432\n",
      "Categorical []\n",
      "# 0  GLOBAL CAIM  0.028294468085986825\n",
      "Categorical []\n",
      "# 0  GLOBAL CAIM  0.05579460993589193\n",
      "Categorical []\n",
      "# 0  GLOBAL CAIM  0.03037694369949622\n",
      "Categorical []\n",
      "# 0  GLOBAL CAIM  0.053089644498094994\n"
     ]
    }
   ],
   "source": [
    "result = runner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result[0].edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
